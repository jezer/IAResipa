{"created": 1752723988.049979, "duration": 17.978960514068604, "exitcode": 1, "root": "C:\\source\\IAResipa", "environment": {}, "summary": {"failed": 4, "passed": 1, "total": 5, "collected": 5}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "tests/07.organizacaofluxo/test_flow_orchestrator.py", "type": "Module"}, {"nodeid": "tests/08.responderaoUsuario/test_response_generator.py", "type": "Module"}]}, {"nodeid": "tests/07.organizacaofluxo/test_flow_orchestrator.py::TestFlowOrchestrator", "outcome": "passed", "result": [{"nodeid": "tests/07.organizacaofluxo/test_flow_orchestrator.py::TestFlowOrchestrator::test_routing_scenarios[Routing_RegisteredUser]", "type": "Function", "lineno": 15}, {"nodeid": "tests/07.organizacaofluxo/test_flow_orchestrator.py::TestFlowOrchestrator::test_routing_scenarios[Routing_UnregisteredUser]", "type": "Function", "lineno": 15}]}, {"nodeid": "tests/07.organizacaofluxo/test_flow_orchestrator.py", "outcome": "passed", "result": [{"nodeid": "tests/07.organizacaofluxo/test_flow_orchestrator.py::TestFlowOrchestrator", "type": "Class"}]}, {"nodeid": "tests/08.responderaoUsuario/test_response_generator.py::TestResponseGenerator", "outcome": "passed", "result": [{"nodeid": "tests/08.responderaoUsuario/test_response_generator.py::TestResponseGenerator::test_scenarios[ClassifyIntent_Success]", "type": "Function", "lineno": 31}, {"nodeid": "tests/08.responderaoUsuario/test_response_generator.py::TestResponseGenerator::test_scenarios[ClassifyIntent_JsonError]", "type": "Function", "lineno": 31}, {"nodeid": "tests/08.responderaoUsuario/test_response_generator.py::TestResponseGenerator::test_scenarios[FormatResponse_Success]", "type": "Function", "lineno": 31}]}, {"nodeid": "tests/08.responderaoUsuario/test_response_generator.py", "outcome": "passed", "result": [{"nodeid": "tests/08.responderaoUsuario/test_response_generator.py::TestResponseGenerator", "type": "Class"}]}], "tests": [{"nodeid": "tests/07.organizacaofluxo/test_flow_orchestrator.py::TestFlowOrchestrator::test_routing_scenarios[Routing_RegisteredUser]", "lineno": 15, "outcome": "failed", "keywords": ["test_routing_scenarios[Routing_RegisteredUser]", "Routing_RegisteredUser", "TestFlowOrchestrator", "parametrize", "test_flow_orchestrator.py", "07.organizacaofluxo", "tests", "IAResipa", ""], "setup": {"duration": 0.002578499959781766, "outcome": "passed"}, "call": {"duration": 3.3283422000240535, "outcome": "failed", "crash": {"path": "C:\\source\\IAResipa\\tests\\07.organizacaofluxo\\test_flow_orchestrator.py", "lineno": 29, "message": "AssertionError: assert 'general_query' == 'check_availability'\n  \n  - check_availability\n  + general_query"}, "traceback": [{"path": "tests\\07.organizacaofluxo\\test_flow_orchestrator.py", "lineno": 29, "message": "AssertionError"}], "stdout": "Erro ao classificar inten\u00e7\u00e3o: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\nExecutando n\u00f3: check_user\n", "log": [{"name": "langchain_google_genai.chat_models", "msg": "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised NotFound: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods..", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "C:\\Program Files\\Python313\\Lib\\site-packages\\tenacity\\before_sleep.py", "filename": "before_sleep.py", "module": "before_sleep", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 65, "funcName": "log_it", "created": 1752723973.5381122, "msecs": 538.0, "relativeCreated": 5564.2403, "thread": 22220, "threadName": "MainThread", "processName": "MainProcess", "process": 11752, "taskName": null}], "longrepr": "self = <test_flow_orchestrator.TestFlowOrchestrator object at 0x00000189462C7B10>\nscenario_data = {'expected_state': {'intent': 'check_availability', 'is_registered': True, 'response_contains': 'Resposta formatada pa...ck_check_user': {'is_registered': True, 'user_id': 456}, 'mock_classify_intent': {'intent': 'check_availability'}, ...}\n\n    def test_routing_scenarios(self, scenario_data):\n        # Arrange\n        initial_state = scenario_data[\"initial_state\"].copy()\n        expected_state = scenario_data[\"expected_state\"]\n    \n        # Act\n        final_state = app.invoke(initial_state)\n    \n        # Assert\n        for key, value in expected_state.items():\n            if key == \"response_contains\":\n                assert value in final_state[\"response\"]\n            else:\n>               assert final_state[key] == value\nE               AssertionError: assert 'general_query' == 'check_availability'\nE                 \nE                 - check_availability\nE                 + general_query\n\ntests\\07.organizacaofluxo\\test_flow_orchestrator.py:29: AssertionError"}, "teardown": {"duration": 0.0007138000801205635, "outcome": "passed"}}, {"nodeid": "tests/07.organizacaofluxo/test_flow_orchestrator.py::TestFlowOrchestrator::test_routing_scenarios[Routing_UnregisteredUser]", "lineno": 15, "outcome": "failed", "keywords": ["test_routing_scenarios[Routing_UnregisteredUser]", "Routing_UnregisteredUser", "TestFlowOrchestrator", "parametrize", "test_flow_orchestrator.py", "07.organizacaofluxo", "tests", "IAResipa", ""], "setup": {"duration": 0.00038460001815110445, "outcome": "passed"}, "call": {"duration": 2.485698700067587, "outcome": "failed", "crash": {"path": "C:\\source\\IAResipa\\tests\\07.organizacaofluxo\\test_flow_orchestrator.py", "lineno": 29, "message": "AssertionError: assert 'Usu\u00e1rio n\u00e3o ...para iniciar.' == 'Usu\u00e1rio n\u00e3o cadastrado.'\n  \n  - Usu\u00e1rio n\u00e3o cadastrado.\n  + Usu\u00e1rio n\u00e3o cadastrado. Responda com 'cadastro' para iniciar."}, "traceback": [{"path": "tests\\07.organizacaofluxo\\test_flow_orchestrator.py", "lineno": 29, "message": "AssertionError"}], "stdout": "Erro ao classificar inten\u00e7\u00e3o: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\nExecutando n\u00f3: check_user\n", "log": [{"name": "langchain_google_genai.chat_models", "msg": "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised NotFound: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods..", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "C:\\Program Files\\Python313\\Lib\\site-packages\\tenacity\\before_sleep.py", "filename": "before_sleep.py", "module": "before_sleep", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 65, "funcName": "log_it", "created": 1752723977.2555525, "msecs": 255.0, "relativeCreated": 9281.6805, "thread": 22220, "threadName": "MainThread", "processName": "MainProcess", "process": 11752, "taskName": null}], "longrepr": "self = <test_flow_orchestrator.TestFlowOrchestrator object at 0x00000189462C7D90>\nscenario_data = {'expected_state': {'is_registered': False, 'response': 'Usu\u00e1rio n\u00e3o cadastrado.'}, 'initial_state': {'phone_number': ...istered': False, 'response': 'Usu\u00e1rio n\u00e3o cadastrado.'}, 'mock_classify_intent': {'intent': 'check_availability'}, ...}\n\n    def test_routing_scenarios(self, scenario_data):\n        # Arrange\n        initial_state = scenario_data[\"initial_state\"].copy()\n        expected_state = scenario_data[\"expected_state\"]\n    \n        # Act\n        final_state = app.invoke(initial_state)\n    \n        # Assert\n        for key, value in expected_state.items():\n            if key == \"response_contains\":\n                assert value in final_state[\"response\"]\n            else:\n>               assert final_state[key] == value\nE               AssertionError: assert 'Usu\u00e1rio n\u00e3o ...para iniciar.' == 'Usu\u00e1rio n\u00e3o cadastrado.'\nE                 \nE                 - Usu\u00e1rio n\u00e3o cadastrado.\nE                 + Usu\u00e1rio n\u00e3o cadastrado. Responda com 'cadastro' para iniciar.\n\ntests\\07.organizacaofluxo\\test_flow_orchestrator.py:29: AssertionError"}, "teardown": {"duration": 0.00044410000555217266, "outcome": "passed"}}, {"nodeid": "tests/08.responderaoUsuario/test_response_generator.py::TestResponseGenerator::test_scenarios[ClassifyIntent_Success]", "lineno": 31, "outcome": "failed", "keywords": ["test_scenarios[ClassifyIntent_Success]", "ClassifyIntent_Success", "TestResponseGenerator", "parametrize", "test_response_generator.py", "08.responderaoUsuario", "tests", "IAResipa", ""], "setup": {"duration": 0.00040660006925463676, "outcome": "passed"}, "call": {"duration": 2.750002000015229, "outcome": "failed", "crash": {"path": "C:\\source\\IAResipa\\tests\\08.responderaoUsuario\\test_response_generator.py", "lineno": 51, "message": "AssertionError: assert 'general_query' == 'check_availability'\n  \n  - check_availability\n  + general_query"}, "traceback": [{"path": "tests\\08.responderaoUsuario\\test_response_generator.py", "lineno": 51, "message": "AssertionError"}], "stdout": "Erro ao classificar inten\u00e7\u00e3o: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n", "log": [{"name": "langchain_google_genai.chat_models", "msg": "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised NotFound: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods..", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "C:\\Program Files\\Python313\\Lib\\site-packages\\tenacity\\before_sleep.py", "filename": "before_sleep.py", "module": "before_sleep", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 65, "funcName": "log_it", "created": 1752723980.1386197, "msecs": 138.0, "relativeCreated": 12164.7477, "thread": 22220, "threadName": "MainThread", "processName": "MainProcess", "process": 11752, "taskName": null}], "longrepr": "self = <test_response_generator.TestResponseGenerator object at 0x00000189464F0190>\nscenario_name = 'ClassifyIntent_Success'\n\n    def test_scenarios(self, scenario_name):\n        \"\"\"Executa um cen\u00e1rio de teste com base nos dados carregados.\"\"\"\n        scenario_data = all_scenarios_data[scenario_name]\n    \n        # Arrange: Prepara o estado inicial e o mock do LLM\n        initial_state = scenario_data[\"initial_state\"].copy()\n        mock_response = scenario_data.get(\"mock_llm_response\")\n        expected_state = scenario_data[\"expected_state\"]\n    \n        # Act: Decide qual fun\u00e7\u00e3o testar com base no nome do cen\u00e1rio\n        if \"ClassifyIntent\" in scenario_name:\n            result_state = classify_intent_with_llm(initial_state)\n        elif \"FormatResponse\" in scenario_name:\n            result_state = format_response_with_llm(initial_state)\n        else:\n            # This should ideally not be reached due to filtering, but as a safeguard\n            pytest.fail(f\"Cen\u00e1rio de teste n\u00e3o reconhecido ou n\u00e3o implementado para test_response_generator.py: {scenario_name}\")\n    \n        for key, value in expected_state.items():\n>           assert result_state[key] == value\nE           AssertionError: assert 'general_query' == 'check_availability'\nE             \nE             - check_availability\nE             + general_query\n\ntests\\08.responderaoUsuario\\test_response_generator.py:51: AssertionError"}, "teardown": {"duration": 0.00042830000165849924, "outcome": "passed"}}, {"nodeid": "tests/08.responderaoUsuario/test_response_generator.py::TestResponseGenerator::test_scenarios[ClassifyIntent_JsonError]", "lineno": 31, "outcome": "passed", "keywords": ["test_scenarios[ClassifyIntent_JsonError]", "ClassifyIntent_JsonError", "TestResponseGenerator", "parametrize", "test_response_generator.py", "08.responderaoUsuario", "tests", "IAResipa", ""], "setup": {"duration": 0.0002762000076472759, "outcome": "passed"}, "call": {"duration": 2.8929469001013786, "outcome": "passed", "stdout": "Erro ao classificar inten\u00e7\u00e3o: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n", "log": [{"name": "langchain_google_genai.chat_models", "msg": "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised NotFound: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods..", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "C:\\Program Files\\Python313\\Lib\\site-packages\\tenacity\\before_sleep.py", "filename": "before_sleep.py", "module": "before_sleep", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 65, "funcName": "log_it", "created": 1752723982.9910314, "msecs": 991.0, "relativeCreated": 15017.1594, "thread": 22220, "threadName": "MainThread", "processName": "MainProcess", "process": 11752, "taskName": null}]}, "teardown": {"duration": 0.0007624999852851033, "outcome": "passed"}}, {"nodeid": "tests/08.responderaoUsuario/test_response_generator.py::TestResponseGenerator::test_scenarios[FormatResponse_Success]", "lineno": 31, "outcome": "failed", "keywords": ["test_scenarios[FormatResponse_Success]", "FormatResponse_Success", "TestResponseGenerator", "parametrize", "test_response_generator.py", "08.responderaoUsuario", "tests", "IAResipa", ""], "setup": {"duration": 0.0006270000012591481, "outcome": "passed"}, "call": {"duration": 2.8284582000924274, "outcome": "failed", "crash": {"path": "C:\\source\\IAResipa\\tests\\08.responderaoUsuario\\test_response_generator.py", "lineno": 51, "message": "AssertionError: assert 'Ocorreu um e...te novamente.' == 'Os seguintes... de reservar?'\n  \n  - Os seguintes quiosques est\u00e3o dispon\u00edveis: Quiosque Sol, Quiosque Lua. Qual voc\u00ea gostaria de reservar?\n  + Ocorreu um erro ao processar sua solicita\u00e7\u00e3o. Tente novamente."}, "traceback": [{"path": "tests\\08.responderaoUsuario\\test_response_generator.py", "lineno": 51, "message": "AssertionError"}], "stdout": "Erro ao formatar resposta: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n", "log": [{"name": "langchain_google_genai.chat_models", "msg": "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised NotFound: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods..", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "C:\\Program Files\\Python313\\Lib\\site-packages\\tenacity\\before_sleep.py", "filename": "before_sleep.py", "module": "before_sleep", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 65, "funcName": "log_it", "created": 1752723985.8804042, "msecs": 880.0, "relativeCreated": 17906.5323, "thread": 22220, "threadName": "MainThread", "processName": "MainProcess", "process": 11752, "taskName": null}], "longrepr": "self = <test_response_generator.TestResponseGenerator object at 0x0000018946482060>\nscenario_name = 'FormatResponse_Success'\n\n    def test_scenarios(self, scenario_name):\n        \"\"\"Executa um cen\u00e1rio de teste com base nos dados carregados.\"\"\"\n        scenario_data = all_scenarios_data[scenario_name]\n    \n        # Arrange: Prepara o estado inicial e o mock do LLM\n        initial_state = scenario_data[\"initial_state\"].copy()\n        mock_response = scenario_data.get(\"mock_llm_response\")\n        expected_state = scenario_data[\"expected_state\"]\n    \n        # Act: Decide qual fun\u00e7\u00e3o testar com base no nome do cen\u00e1rio\n        if \"ClassifyIntent\" in scenario_name:\n            result_state = classify_intent_with_llm(initial_state)\n        elif \"FormatResponse\" in scenario_name:\n            result_state = format_response_with_llm(initial_state)\n        else:\n            # This should ideally not be reached due to filtering, but as a safeguard\n            pytest.fail(f\"Cen\u00e1rio de teste n\u00e3o reconhecido ou n\u00e3o implementado para test_response_generator.py: {scenario_name}\")\n    \n        for key, value in expected_state.items():\n>           assert result_state[key] == value\nE           AssertionError: assert 'Ocorreu um e...te novamente.' == 'Os seguintes... de reservar?'\nE             \nE             - Os seguintes quiosques est\u00e3o dispon\u00edveis: Quiosque Sol, Quiosque Lua. Qual voc\u00ea gostaria de reservar?\nE             + Ocorreu um erro ao processar sua solicita\u00e7\u00e3o. Tente novamente.\n\ntests\\08.responderaoUsuario\\test_response_generator.py:51: AssertionError"}, "teardown": {"duration": 0.0005094000371173024, "outcome": "passed"}}], "warnings": [{"message": "The 'timeout' parameter is deprecated. Please configure it in the http client instead.", "category": "DeprecationWarning", "when": "runtest", "filename": "C:\\Program Files\\Python313\\Lib\\site-packages\\supabase\\_sync\\client.py", "lineno": 303}, {"message": "The 'verify' parameter is deprecated. Please configure it in the http client instead.", "category": "DeprecationWarning", "when": "runtest", "filename": "C:\\Program Files\\Python313\\Lib\\site-packages\\supabase\\_sync\\client.py", "lineno": 303}]}