Perfeito, vou montar uma instrução otimizada para que uma IA coordenadora use a API do Gemini, organize o fluxo entre múltiplos agentes (via Azure Functions ou endpoints Python), armazene o contexto no Supabase e permita adicionar novos agentes facilmente. Te trago já já um modelo eficiente e performático.


# Coordenando Múltiplos Agentes de IA com um Agente Orquestrador (Gemini e Supabase)

## Visão Geral da Arquitetura Multi-Agente

Dividir uma aplicação de IA em **múltiplos agentes especializados** coordenados por um agente principal (o *orquestrador*) é uma forma eficaz de lidar com tarefas complexas. Em vez de um único agente monolítico difícil de manter, temos um agente coordenador central (tipicamente um LLM, como o Google Gemini) que **decide qual agente chamar** para cada solicitação do usuário. Esse **agente coordenador** atua como o “cérebro” do sistema: ele interpreta as instruções do usuário, consulta o contexto armazenado e delega subtarefas a cerca de 20 (ou mais) agentes especializados conforme necessário. Cada agente especializado foca em uma função específica (por exemplo, criar pedido, cancelar pedido, verificar estoque, etc.), enquanto o coordenador garante que eles colaborem de forma coerente para atingir o objetivo geral.

Uma arquitetura assim centralizada simplifica a tomada de decisões, pois o coordenador enxerga o panorama geral e encaminha cada tarefa ao módulo certo. (Por exemplo, um agente de pagamento lida apenas com cobranças, outro apenas com suporte técnico, e assim por diante.) A contrapartida é que o coordenador se torna um ponto único de controle; portanto, é importante projetá-lo para ser robusto e escalável. Em sistemas multi-agente tradicionais, esse padrão é chamado de **“Coordenador/Dispatcher”**, onde um agente central roteia pedidos para agentes especialistas. Em suma, o coordenador recebe o comando do usuário e **escolhe o agente apropriado** para executá-lo, garantindo que cada solicitação seja tratada pelo especialista certo.

## Implementação dos Agentes Especializados (Questão 1)

**Como implementar os agentes?** Uma abordagem comum é codificar cada agente como uma função ou microserviço Python independente, exposto via API (por exemplo, endpoints REST em FastAPI ou funções serverless no Azure Functions). Assim, o agente coordenador pode invocar cada agente seja chamando a função Python diretamente (se estiver no mesmo processo) ou enviando requisições HTTP para o endpoint correspondente. Separar os agentes dessa forma facilita a manutenção e a adição de novos agentes no futuro – basta desenvolver a nova função e registrá-la para que o coordenador passe a conhecê-la.

No seu caso, usando a **API do Gemini** como motor de IA, o agente coordenador em si seria alimentado por um modelo Gemini. O Gemini suporta *function calling*, isto é, permite ao modelo chamar funções definidas pelo desenvolvedor durante a conversa. Podemos aproveitar esse recurso definindo cada agente especializado como uma “função” disponível para o modelo coordenador. **Exemplo:** registramos funções como `criar_pedido(pedido_dados)`, `cancelar_pedido(pedido_id)`, etc., e fornecemos ao Gemini as assinaturas e descrições dessas funções. Com isso, o modelo LLM coordenador pode **escolher autonomamente chamar a função correta** de acordo com a instrução do usuário. Essa técnica, semelhante ao que a OpenAI faz com ferramentas, permite que a IA orquestradora integre-se perfeitamente com APIs externas ou lógicas Python customizadas, executando ações no mundo real em nome do usuário.

Outra opção é implementar um **servidor central (usando FastAPI)** que receba as solicitações dos usuários e atue como coordenador. Esse servidor consultaria um módulo de roteamento (possivelmente alimentado por IA ou regras fixas) para determinar qual endpoint de agente chamar. Por exemplo, com FastAPI você poderia ter rotas como `/agent/criar_pedido` mapeadas para funções Python correspondentes. O coordenador (em FastAPI ou outra estrutura) então chamaria internamente `await criar_pedido(dados)` ou externamente via HTTP se os agentes estiverem distribuídos. Essa arquitetura de **microsserviços** permite escalar cada agente independentemente e até implementá-los em linguagens diferentes se necessário, contanto que exponham uma API consistente.

Importante notar que **descrições claras e padronizadas dos agentes** são fundamentais. Cada agente deve ter uma descrição do que faz (por exemplo: *"Agente de Cobrança: lida com questões de pagamento"*; *"Agente de Suporte: resolve problemas técnicos"*). Essas descrições ajudam o orquestrador (seja um modelo LLM ou lógica customizada) a decidir corretamente quem chamar. No padrão de coordenador orientado por LLM, recomenda-se fornecer essas descrições e uma instrução explícita no agente coordenador para garantir o roteamento correto. Por exemplo, a documentação do Google ADK mostra um coordenador Gemini configurado assim: *“Route user requests: Use Billing agent for payment issues, Support agent for technical problems.”* (tradução: *"Encaminhe as solicitações: use o agente de Cobranças para problemas de pagamento e o agente de Suporte para problemas técnicos"*). Essa instrução embutida no prompt do coordenador deixa claro, para a IA, qual agente chamar em cada caso.

Em resumo, os agentes podem ser implementados como **funções Python moduladas ou endpoints de API**. O orquestrador (modelo LLM Gemini) pode chamá-los via *function calls* nativamente ou um serviço intermediário em Python pode fazer o roteamento. O importante é que a interface de cada agente seja bem definida (inputs/outputs conhecidos) para que a coordenação ocorra de forma confiável.

## Agente Coordenador com Memória de Contexto (Questão 2)

Para que o coordenador tome decisões informadas em chamadas subsequentes (segunda interação em diante), é crucial **manter um histórico de contexto**. A solução proposta é usar o **Supabase** (que oferece um banco de dados PostgreSQL) como repositório dessa memória de longo prazo. Assim, mesmo que cada consulta ao modelo LLM seja independente, o orquestrador pode **armazenar e recuperar informações de estado** no Supabase para ter continuidade.

Como implementar isso? Existem algumas camadas de memória que podem ser úteis:

* **Histórico de mensagens (memória de curto prazo)**: gravar no Supabase cada mensagem do usuário e resposta do sistema, para recuperar o diálogo recente. Isso garante que, ao receber uma nova ordem, o coordenador possa ver o que já foi feito recentemente (por exemplo: "Pedido #123 criado no passo anterior").
* **Memória semântica (vetorial)**: opcionalmente, usar a extensão pgvector do Supabase para armazenar embeddings das interações e permitir busca semântica. Isso ajuda a encontrar fatos mencionados anteriormente mesmo que o usuário não se refira explicitamente (ex: *"aquele produto que pedi mês passado"*). Essa técnica de vetor semântico foi demonstrada em exemplo da Supabase, onde o histórico de conversa é enriquecido com um índice vetorial para *“fuzzy retrieval”* de informações mencionadas.
* **Memória estruturada (dados tabulares)**: armazenar dados estruturados relevantes em tabelas PostgreSQL. No caso de um sistema de pedidos, por exemplo, o agente de criação poderia salvar um registro na tabela `pedidos` (com ID, item, status, etc.). O coordenador ou outros agentes podem então consultar essa tabela via SQL (diretamente, ou via uma função ferramenta de consulta) para obter detalhes. Esse tipo de memória estruturada garante **precisão** quando o usuário pergunta algo específico (*"Qual o status do meu pedido #123?"* pode ser respondido buscando essa linha no DB).

Usando Supabase, você consegue **unificar essas camadas de memória num só lugar** (o Postgres). Conforme citado no blog da própria Supabase, uma solução de assistente de IA robusta combina *histórico de mensagens, memória vetorial e armazenamento SQL* para alcançar continuidade de conversa. Em frameworks de agentes, a memória bem estruturada **permite que múltiplos agentes consultem informações compartilhadas** de forma eficiente. Isso é fundamental num arranjo multi-agente: o que um agente faz (ex: cria um pedido) precisa estar acessível a outro agente depois (ex: um agente de cancelamento precisa saber o ID do pedido criado para poder cancelá-lo). **Memória compartilhada** resolve isso. No contexto de LLMs, já foi observado que memória persistente é crucial para manter a continuidade em sistemas de agentes colaborativos, evitando perda de contexto entre passos.

Concretamente, o agente coordenador pode interagir com o Supabase de algumas maneiras:

* **Consulta direta pelo código**: antes de chamar o modelo Gemini, o coordenador (a aplicação Python) pode consultar o Supabase para buscar informações contextuais relevantes e incluir esses dados no prompt do modelo. Por exemplo: adicionar ao prompt algo como *"Contexto: Pedido\_atual\_id=1234, status=CRIADO"* se isso for relevante para a próxima ação.
* **Função de memória como ferramenta**: Alternativamente, você pode expor uma função/tool ao LLM para consultar a base (por exemplo, `consultar_memoria(chave)` que retorna um valor do DB). O Gemini então poderia decidir chamar `consultar_memoria("pedido_atual")` quando precisar, e seu código Python implementaria essa função acessando o Supabase. Essa abordagem mantém a decisão dentro do LLM, mas dá controle seguro sobre o que exatamente é consultado.
* **Atualização do contexto**: Sempre que um agente especializado executar uma ação, o coordenador deve **persistir os resultados** ou mudanças de estado no Supabase. Por exemplo, após o agente de criação de pedido responder com “Pedido 1234 criado com sucesso”, o coordenador salva no banco algo como `ultima_acao="pedido_criado"` e `ultimo_pedido_id=1234`. Assim, se o usuário em seguida diz "cancele o pedido", o coordenador já sabe qual pedido cancelar consultando esses campos.

Em suma, o **Supabase funcionará como a memória de longo prazo** do agente coordenador, garantindo que o contexto de interações anteriores esteja disponível a cada nova etapa. Essa persistência complementa a *janela de contexto* do LLM (que embora grande – o Gemini 2.5 suporta milhões de tokens – não deve depender apenas dela, por confiabilidade). Conforme apontado por IBM, uma memória bem projetada permite consistência e uso de experiências passadas pelo agente, resultando em respostas mais informadas e coordenadas entre vários agentes.

## Seleção do Agente e Fluxo de Execução (Questão 3 parcialmente)

O **fluxo de decisão do agente coordenador** pode ser descrito passo-a-passo, levando em conta se é a primeira ordem do usuário ou uma subsequente. Abaixo detalhamos esse processo lógico, alinhado com o exemplo dado (criação de pedido seguida de cancelamento):

1. **Receber a solicitação do usuário**: O usuário emite um comando (ex: *"Quero fazer um pedido do produto X"*). O sistema registra essa solicitação e a entrega ao agente coordenador (Gemini) junto com a instrução do sistema (veremos adiante) e, se houver, contexto prévio.
2. **Analisar o tipo de solicitação (roteamento inicial)**: Se **for a primeira ordem da conversa** (ou seja, nenhum passo anterior existe), o coordenador deve **identificar qual agente especialista é apropriado** para atender o pedido. Ele faz isso interpretando a intenção do usuário. Por exemplo, “fazer um pedido do produto X” indica uma ação de **criação de pedido**, então o coordenador deve chamar o **AgenteDePedido**. Esta identificação pode ser implementada de forma *rule-based* (por meio de palavras-chave ou intenções pré-definidas) ou via o próprio LLM utilizando seu poder de classificação semântica. Os modelos Gemini possuem forte capacidade de **raciocínio lógico e planejamento**, conseguindo **quebrar tarefas complexas em passos menores**. Ou seja, o coordenador LLM pode, a partir da instrução do usuário, *planejar* que a primeira etapa é “usar o agente X para fazer Y”. *(Em frameworks como HuggingGPT e LangChain ReAct, esse é o momento de **planejar** qual ferramenta/ação vem em seguida com base na tarefa solicitada.)*
3. **Consultar contexto (se aplicável)**: Se **não é a primeira interação** – por exemplo, o usuário disse antes "faça pedido" e agora diz "cancele o pedido" – o coordenador precisa **levar em conta o estado atual**. Nesse caso, ele consultaria o Supabase para recuperar informações relevantes do histórico: ele encontraria que o último pedido criado tem ID 1234 e está ativo. Com esse contexto em mãos, o coordenador entende que “cancele o pedido” refere-se ao pedido *1234* recém criado. Essa etapa de recuperação de memória garante que o coordenador não atue no vazio, mas sim com consciência do que já ocorreu.
4. **Selecionar o agente apropriado para a nova solicitação**: Agora, com a intenção do usuário (e possivelmente detalhes do contexto) claramente identificados, o coordenador escolhe **qual agente chamar** em seguida. No exemplo, após criar um pedido no passo 1, a segunda solicitação *"cancelar o pedido"* será roteada ao **AgenteDeCancelamento** (especialista em cancelamentos), passando a ele o ID do pedido a cancelar. O mapeamento entre intenções e agentes pode estar parcialmente codificado na própria **instrução do coordenador** (por exemplo: "se o usuário falar em cancelar, usar AgenteCancelamento") ou ser decidido dinamicamente pelo LLM. Muitas arquiteturas optam por uma **pergunta ao LLM do tipo “qual agente usar?”**. Por exemplo, o coordenador LLM pode ser instruído a responder com um campo `next_agent` indicando qual agente escolher. Essa resposta estruturada é então lida pelo código para despachar a chamada. De fato, no LangChain/LangGraph, muitas vezes força-se o modelo a produzir uma saída JSON com o próximo passo, facilitando o roteamento programático.
5. **Refinar a instrução e chamar o agente especialista**: Antes de invocar o agente escolhido, o coordenador pode **melhorar ou complementar a solicitação** para torná-la mais executável. Esse refinamento inclui: adicionar dados de contexto (ex: inserir o ID do pedido na requisição de cancelamento), esclarecer detalhes vagos do usuário e formatar a ordem de forma estruturada. O primeiro agente (coordenador) atua quase como um *planejador/tradutor* da vontade do usuário para uma ação concreta do sistema. Uma vez definido o comando específico, o coordenador efetua a chamada – se for via *function call* do LLM, o próprio modelo retorna a chamada para a função do agente; se for via API, o código do coordenador faz a requisição HTTP ao serviço do agente.
6. **Receber o resultado e registrar mudanças**: O agente especializado executa sua tarefa (por exemplo, o AgenteDePedido cria o pedido no sistema e retorna *"Pedido 1234 criado com sucesso"*). O coordenador então toma essa resposta e pode tanto entregá-la ao usuário imediatamente quanto usar para determinar próximos passos. Independentemente disso, **é vital registrar o resultado no Supabase** (atualizar a memória): gravar que o pedido 1234 foi criado, status atual etc. para referência futura. Se a interação continua (usuário faz outro pedido ou pergunta algo), esse dado estará disponível.
7. **Iterar conforme novas ordens chegam**: A cada nova entrada do usuário, o processo se repete: o coordenador olha o pedido, avalia contexto, escolhe agente e assim por diante. O sistema pode suportar fluxos multi-passos complexos, e inclusive **vários agentes colaborando em sequência**. Por exemplo, para uma tarefa complexa (*"Quero devolver um produto X e ser reembolsado"*), o coordenador poderia chamar em sequência: AgenteDevolução -> AgenteLogística -> AgenteFinanceiro, cada um cuidando de uma parte. O coordenador atuaria como **gerente de fluxo**, garantindo que o output de um vire input do próximo (encadeamento), e informando o usuário do progresso. Esse estilo de **pipeline sequencial** é comum em multi-agentes, onde o resultado de um agente é compartilhado via estado para o seguinte. Novamente, o Supabase ou o estado interno cumpre o papel de armazenar esses resultados intermediários de forma que os próximos passos tenham acesso.

Resumidamente, o **agente coordenador** avalia continuamente *“Qual é o próximo melhor passo? Quem (qual agente) deve executá-lo?”*, usando tanto regras definidas quanto o próprio raciocínio do LLM. Graças às capacidades avançadas do Gemini em **planejamento e raciocínio**, ele pode decompor solicitações do usuário em tarefas menores e escolher a ordem apropriada de execução. E com as **chamadas de função nativas** do modelo, ele consegue acionar os agentes como se fossem ferramentas, realizando as ações necessárias no mundo externo. Tudo isso, sustentado por uma camada de memória (Supabase), resulta num fluxo de trabalho automatizado onde vários agentes de IA cooperam para realizar o objetivo do usuário com contexto consistente.

## Frameworks e Ferramentas Sugeridas (Questão 3 e Adição de Novos Agentes)

Para implementar a arquitetura descrita de forma eficiente e **facilitar a adição de novos agentes**, vale considerar frameworks e ferramentas já existentes que suportam sistemas multi-agente e orquestração de LLMs:

* **LangChain**: biblioteca open-source popular para orquestração de LLMs. Ela permite definir *tools* (ferramentas) que um agente LLM pode usar. No seu caso, cada agente especializado pode ser registrado como uma *tool* com nome e função, e você pode então usar um agente do tipo ReAct ou autônomo do LangChain para decidir qual tool chamar a cada passo. O LangChain cuida de chamar o modelo com um prompt adequado (que inclui descrições das ferramentas disponíveis) e interpretar sua resposta. É bastante flexível e já integra com memória (incluindo pgvector) e com APIs diversas. Como é gratuito e em Python, encaixa-se bem no seu stack.

* **Google Agent Development Kit (ADK)**: uma iniciativa da Google (open-source) voltada especificamente para construir agentes com modelos como o Gemini. O ADK suporta composição hierárquica de agentes (coordenadores e sub-agentes) e fornece primitivas de comunicação entre eles. No exemplo que citamos, eles definem um *LlmAgent* coordenador (usando `gemini-2.0`) e vários sub-agentes especializados, e o ADK lida com o mecanismo de *delegation* (transferência de controle) entre eles. Esse kit facilita a implementação de padrões como o **Coordinator/Dispatcher**, pipelines sequenciais, paralelismo, etc., com pouca configuração adicional além de definir os *agents* e suas relações. Pode ser uma boa opção se você estiver focado no ecossistema Google.

* **Open-Source Multi-Agent Frameworks (AutoGen, etc.)**: A Microsoft Research lançou o **AutoGen**, um framework open-source para construir agentes conversando entre si. Ele permite criar agentes de chat (baseados em LLMs) que colaboram, possivelmente útil se você quiser modelar interações do tipo *gerente* e *trabalhadores* (um agente gerente divide tarefas e delega a agentes trabalhadores, similar ao seu coordenador/especialistas). A vantagem de frameworks como esse é manejar automaticamente as trocas de mensagens entre agentes, concorrência e até intervenções humanas, sem que você precise implementar todo o protocolo de comunicação.

* **Crew**<sup>AI</sup> e outros\*\*:\*\* O blog do Google destaca ferramentas como o **CrewAI**, que é voltado para **orquestrar agentes autônomos colaborativos** de forma simplificada. O CrewAI permite definir agentes com papéis e objetivos específicos (até com “backstories”) e coordenar a colaboração entre eles, integrando-se nativamente aos modelos Gemini. Ele abstrai boa parte do trabalho de coordenação, o que pode acelerar o desenvolvimento. Outras bibliotecas citadas incluem **LangGraph** (para modelar fluxos multi-agente como um grafo de estados/etapas) e **LlamaIndex** (para agentes focados em interação com dados/documentos) – a escolha depende do caso de uso. Todas essas soluções possuem versões de código aberto ou planos gratuitos para experimentação, alinhando-se ao requisito de usar ferramentas *free* quando possível.

* **FastAPI (ou frameworks web)**: Se preferir construir sem abstrações de agente de alto nível, você pode usar um microframework web como o FastAPI para rapidamente criar endpoints para cada agente e um endpoint para o coordenador. O FastAPI é eficiente, fácil de usar e gratuito. Com ele, você estruturaria seu projeto por rotas: por exemplo, `POST /acao/criar_pedido` para o agente de criar pedido, `POST /acao/cancelar_pedido` para o de cancelar, etc., cada uma chamando a função Python adequada. O agente coordenador poderia ser implementado como um *controller* que recebe as requisições do cliente (usuário final) e internamente faz chamadas HTTP para os outros endpoints ou chamadas diretas a funções (se no mesmo serviço) conforme a lógica de decisão. Essa abordagem “faça você mesmo” dá mais controle e transparência do fluxo, e com a ajuda de bibliotecas como `requests` (para chamadas HTTP) ou até integração direta via chamadas de função, você consegue manter o sistema modular. Para adicionar um novo agente, bastaria criar um novo endpoint/função e atualizar o mapeamento de decisões do coordenador (ou sua prompt/instrução, se for decidido via LLM).

Em termos de escolha, se seu foco é velocidade de prototipação e robustez, usar algo como LangChain Agents ou ADK (dado que você usará Gemini) pode adiantar muito trabalho, pois eles já tratam de parsing de respostas do LLM, formatos de saída, integração com memória do LangChain (que poderia usar o Supabase como armazenamento via PGVector, por exemplo) e até streaming de tokens. Por outro lado, entender e implementar manualmente com FastAPI+Gemini API te dá mais noção do que acontece e possivelmente menos overhead. Uma estratégia híbrida poderia ser: prototipar com LangChain para validar a lógica multi-agente e depois, se desejar otimização, codificar manualmente a versão final.

## Formato da Instrução do Agente Coordenador (Questão 4)

A **“instrução”** a ser fornecida ao agente coordenador (isto é, o prompt ou sistema que o modelo LLM Gemini receberá) deve ser escrita de maneira **clara, estruturada e abrangente**. O objetivo é garantir que, ao ler essa instrução, a IA entenda perfeitamente seu papel de orquestradora e como proceder em diversas situações. Seguem algumas recomendações sobre o formato e conteúdo dessa instrução:

* **Linguagem natural com bullet points ou passos numerados:** LLMs tendem a seguir bem instruções em texto claro. Você pode escrever a instrução em linguagem natural (português ou inglês conforme preferência e melhor desempenho do modelo – muitos desenvolvedores notam que modelos grandes têm desempenho ótimo em inglês, mas o Gemini deve ser multilíngue também). Organize as regras e orientações em forma de lista (itens numerados ou marcadores) para facilitar a leitura "passo a passo" pelo modelo. Instruções concisas e enumeradas evitam ambiguidades. Por exemplo, ao invés de um parágrafo corrido descrevendo tudo, divida em itens como "1. Se for a primeira pergunta do usuário, faça X...", "2. Se já houver contexto, faça Y...". Isso torna explícito o fluxo condicional que o agente deve seguir.

* **Especificar o papel e os recursos disponíveis:** Comece definindo o papel do agente: *"Você é um agente coordenador capaz de chamar outros 20 agentes especialistas."* Liste quais são esses agentes e suas funções resumidamente, pois isso ajuda o modelo a relacionar solicitações a agentes. (Por exemplo: *"Agentes disponíveis: \[Pedido] cria novos pedidos; \[Cancelamento] cancela pedidos existentes; \[SuporteCliente] responde dúvidas gerais; ..."*). Essas **descrições dos sub-agentes devem ser bem claras** sobre o que cada um faz, já que o LLM usará isso para decidir a quem delegar.

* **Regras de decisão:** Inclua instruções explícitas de **como decidir qual agente chamar**. Isso pode incorporar lógica de primeira interação vs. subsequentes. Por exemplo: *"Se não há histórico prévio (primeira pergunta do usuário), identifique pela solicitação qual agente se encaixa melhor e encaminhe a tarefa para ele."* e *"Se já houver um histórico (passos anteriores realizados), considere o contexto atual: determine se o usuário está se referindo a algo que já foi feito ou dito. Em caso afirmativo, selecione o agente que lide com aquele contexto."* No seu caso, você pode especificar: *"ex: se um pedido foi criado e o usuário pedir para cancelar, chame o agente de cancelamento."* Deixar essas orientações escritas faz com que a IA siga o protocolo esperado, em vez de ter que “adivinhar” o procedimento.

* **Uso da memória (Supabase):** Deixe claro que o agente coordenador **deve usar a memória de contexto**. Por exemplo: *"Sempre que necessário, recupere do banco de dados (Supabase) informações relevantes do histórico, como o último pedido criado, para auxiliar na decisão."* e *"Atualize o histórico no banco após cada ação bem-sucedida, para refletir o novo estado."* Embora o modelo por si só não se conecte ao banco sem uma ferramenta, essa instrução lembra o desenvolvedor (ou o próprio modelo caso tenha ferramentas de DB disponíveis) da importância do passo. Se você implementou uma função-ferramenta tipo `buscar_contexto(chave)`, informe que ele pode usá-la quando precisar de detalhes.

* **Formato da saída ou próxima ação:** Se você deseja que o coordenador devolva um resultado estruturado (por exemplo, indicando qual agente ele decidiu usar e por quê), pode incluir na instrução um template de resposta. Contudo, muitas vezes, como esse agente coordenador não interage diretamente com o usuário final (ele chama o agente e obtém resposta), o foco é mais em *ações* do que em *respostas em linguagem natural*. Ainda assim, você pode dizer: *"Não responda diretamente ao usuário, a não ser que seja um feedback final. Seu principal objetivo é determinar e invocar o agente correto."* Isso impede que o LLM “fale demais” ao invés de agir. Caso use função nativa, o modelo retornará uma chamada de função automaticamente ao escolher a ferramenta, o que já é estruturado. Mas se por algum motivo não estiver usando esse recurso, você poderia pedir algo como: *"Responda apenas com o nome do agente a acionar e os parâmetros necessários, sem informações adicionais."* Esse seria um formato estilo JSON, por exemplo: `{"acao":"cancelar_pedido", "pedido_id":1234}`. Isso torna mais confiável o parsing pelo seu sistema. (Inclusive, conforme práticas recomendadas, outputs estruturados facilitam o orquestrador interpretar a decisão do LLM.)

* **Teste e evolução contínua:** Por fim, observe que a “melhor instrução” pode requerer refinamento com o tempo. É útil rodar alguns testes com diferentes formulações e ver com qual o LLM se sai melhor, ajustando a instrução conforme necessário. Documente preferencialmente a instrução em um formato fácil de atualizar (pode ser um arquivo YAML/JSON de configuração externa ou no próprio código), para que adicionar novos agentes signifique apenas atualizar essa parte.

### Exemplo de Instrução (Prompt) para o Agente Coordenador

Para consolidar as ideias, segue um **exemplo de instrução** em português, estruturada como você poderia fornecer ao modelo Gemini coordenador. Suponha que tenhamos alguns agentes: *AgentePedido*, *AgenteCancelamento* e *AgenteSuporte* para ilustrar:

```markdown
Você é o **Agente Coordenador** de um sistema com múltiplos agentes de IA especializados. 
Seu objetivo é analisar a solicitação do usuário, escolher o agente adequado e passar a ele uma instrução clara para executar a tarefa. Siga rigorosamente as regras abaixo:

1. **Identifique o Agente Correto:** Temos os seguintes agentes disponíveis:  
   - **AgentePedido** – cria um novo pedido com base nos detalhes fornecidos (produto, quantidade, endereço etc.).  
   - **AgenteCancelamento** – cancela um pedido existente, dado um ID de pedido ou detalhes da compra a ser cancelada.  
   - **AgenteSuporte** – lida com dúvidas gerais ou problemas técnicos relatados pelo usuário.  
   *_(Novos agentes podem ser adicionados a esta lista no futuro; sempre considere a lista atualizada de agentes disponíveis.)_*

2. **Primeira Solicitação do Usuário:** Se esta for a **primeira interação** da conversa (nenhum contexto prévio registrado), deduza pela intenção do usuário qual agente é o mais apropriado.  
   - Exemplo: se o usuário solicitar "realizar uma compra" ou "fazer um pedido", encaminhe para **AgentePedido**.  
   - Se solicitar "cancelar meu pedido" logo de início, encaminhe para **AgenteCancelamento** (o agente ainda tentará encontrar pelo menos um pedido associado para cancelar).  
   - Em caso de dúvida genérica ou suporte necessário (ex: "não consigo logar", "como está meu pedido?"), encaminhe para **AgenteSuporte**.  

3. **Solicitações Subsequentes (Com Contexto):** Se já houve passos anteriores, utilize o **contexto armazenado** para guiar a decisão:  
   - Recupere do banco de dados (Supabase) informações relevantes, como detalhes do último pedido criado, último ID de pedido manipulado, status atual da sessão, etc.  
   - **Exemplo:** Se o usuário já criou um pedido numa etapa anterior e agora pede para "cancelar o pedido", identifique qual pedido ele se refere (provavelmente o último criado, a menos que especifique outro) e acione **AgenteCancelamento** passando o ID correspondente.  
   - Se o usuário faz uma pergunta ou instrução relacionada a algo já tratado (por exemplo, pede atualização sobre um pedido existente), chame o agente responsável por aquele assunto (nesse caso, possivelmente **AgenteSuporte** para rastrear o pedido, ou **AgentePedido** se for editar algo no pedido).

4. **Fornecer Instrução ao Agente Especialista:** Após decidir qual agente chamar, **prepare uma instrução clara** para ele. Inclua todos os detalhes que o agente vai precisar, refinando ou traduzindo o pedido do usuário se necessário:  
   - Se informações importantes estiverem no contexto (por ex., ID de pedido, nome de usuário, preferências), insira-as na instrução.  
   - Formule a instrução de forma objetiva e completa, como se fosse o próprio usuário dando uma ordem direta ao agente especialista.  
   - Exemplo de passagem de instrução refinada: o usuário diz "Quero cancelar minha compra anterior". O coordenador já identificou o pedido `1234` no contexto, então ele chamará o AgenteCancelamento com uma mensagem como: "*Cancelar o pedido de ID 1234. O cliente informou que é uma compra recente e deseja o cancelamento.*"

5. **Manter e Atualizar Contexto:** A cada ação realizada:  
   - Registre no Supabase os detalhes importantes (por exemplo, novo pedido criado: salve o ID, itens, data; pedido cancelado: marque status cancelado, etc.).  
   - Assim, o **histórico permanece atualizado**. Use sempre esses dados atualizados para responder perguntas futuras ou decidir próximos passos.  
   - Nunca descarte informações relevantes sem armazená-las, pois podem ser necessárias se o usuário se referir a elas depois.

6. **Respostas e Continuidade:** Em geral, o Agente Coordenador **não responde diretamente ao usuário final**, a menos que a tarefa esteja completa e seja hora de apresentar um resultado unificado. Seu foco principal é coordenar os agentes. Contudo, se nenhum agente especializado for necessário (por exemplo, usuário apenas cumprimenta ou faz uma pergunta simples de resposta direta), você pode responder você mesmo de forma educada ou acionar o AgenteSuporte conforme o caso.  
   - Quando todos os passos necessários forem concluídos (por exemplo, pedido feito ou cancelamento confirmado), você pode então fornecer uma resposta final ao usuário resumindo o que foi feito ou o resultado obtido.

7. **Formato e Precisão:** Siga estas diretrizes adicionais ao longo do processo:  
   - Seja **determinístico e claro** na escolha do agente – não deixe margem para dúvida.  
   - **Não revele** ao usuário detalhes internos (como "vou acionar o Agente X agora"); simplesmente faça a ação e depois informe o resultado relevante.  
   - Garanta que a **comunicação entre agentes** (instruções passadas) seja completa e inequívoca, para evitar idas e vindas desnecessárias.  
   - Mantenha um tom profissional e objetivo nas instruções internas e verifique se o resultado devolvido pelo agente atende ao solicitado; caso contrário, reitere ou ajuste a solicitação.

Com essas regras, você (Agente Coordenador) irá coordenar eficientemente o trabalho dos demais agentes e garantir que a experiência do usuário seja fluida, mesmo em múltiplas etapas.
```

Acima, mostramos um possível template de prompt para o coordenador. Observe que ele está em Markdown apenas para fins de formatação nesta explicação, mas ao usar na API do Gemini você o enviaria como texto normal (pode manter bullet points, pois o modelo entenderá). Você pode adaptá-lo conforme mais agentes sejam adicionados – por exemplo, acrescentando novas entradas na lista de agentes disponíveis e as regras correspondentes de roteamento. O importante é que a instrução permaneça **bem estruturada e fácil de atualizar**, tornando prático incorporar novos agentes no fluxo simplesmente editando essa configuração/prompt, sem ter que refatorar todo o código.

Em conclusão, a chave para sucesso em um sistema de agentes chamando agentes é **projetar um agente coordenador inteligente e bem instruído**. Com uma **boa instrução (prompt de sistema)** delineando claramente suas responsabilidades, ferramentas (agentes) à disposição e protocolos de decisão, a IA coordenadora (alimentada pelo Gemini) será capaz de gerenciar autonomamente os fluxos de tarefas. Aliando isso a um **armazenamento de contexto robusto** no Supabase, você obtém memória consistente entre interações, permitindo ao sistema lidar com sequências de ações e referências passadas. Frameworks open-source como LangChain, ADK ou AutoGen podem agilizar sua implementação, mas mesmo sem eles, seguindo as melhores práticas acima e com uma instrução bem elaborada, você conseguirá uma arquitetura extensível onde adicionar um novo agente é tão simples quanto registrar sua função e atualizar a lista de agentes que o coordenador conhece. Boa sorte no desenvolvimento do seu orquestrador de agentes de IA!

**Referências Utilizadas:**

* IBM Developer Tutorial – *LLM Agent Orchestration* (conceitos de memória, planejamento e ação em agentes autônomos).
* Google Developers Blog – *Building agents with Google Gemini* (vantagens do Gemini para agentes e frameworks de orquestração como LangChain/LangGraph e CrewAI).
* Google ADK Documentation – *Multi-agent systems (Coordinator pattern)* (exemplo de implementação de coordenador com sub-agentes e instrução clara usando Gemini).
* Supabase Blog – *Build a Personalized AI Assistant with Postgres* (uso de PostgreSQL/pgvector para manter histórico de conversas e memórias de longo prazo).
* LangChain/LangGraph Docs – *Structured tool use for agent decision* (forçando saída com próximo agente a usar).
